import json
import pymongo
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

from pydantic import BaseModel, Field

from common.data import MONGO_URI, DB_NAME


# Pydantic ëª¨ë¸ ì •ì˜
class FullDocumentModel(BaseModel):
    """ì „ì²´ ë¬¸ì„œ í˜•íƒœì˜ ìì—°ì–´ ë³€í™˜ ë°ì´í„° ëª¨ë¸"""

    id: str = Field(..., alias="_id")  # ...: í•„ìˆ˜í•„ë“œ, MongoDB: _idë¡œ ì €ì¥/ì¡°íšŒ
    product_code: str
    product_name: str
    content_natural: str  # ë²¡í„° ê²€ìƒ‰ìš©(ë¬¸ì¥í˜•)
    content_structured: str  # llm ë¶„ì„ìš©(êµ¬ì¡°í™”)

    # _id, id í˜¸í™˜ ê°€ëŠ¥
    class Config:
        populate_by_name = True
        validate_by_name = True


class ChunkModel(BaseModel):
    """ì²­í¬ ë‹¨ìœ„ì˜ ìì—°ì–´ ë³€í™˜ ë°ì´í„° ëª¨ë¸ (product ì •ë³´ ì œì™¸)"""

    # basic_info: ê¸°ë³¸ì •ë³´(ìƒí’ˆëª…, ì€í–‰, ê¸ˆë¦¬, ìœ í˜•, ê°€ì…ë°©ì‹)
    # product_guide: ê°€ì…ì •ë³´(ëŒ€ìƒ, ë°©ë²•, í•œë„)
    # basic_rate_info: ê¸°ë³¸ ê¸ˆë¦¬(ì˜ˆì¹˜ê¸ˆì•¡ë³„, ê¸°ê°„ë³„ ì°¨ë“±ê¸ˆë¦¬)
    # preferential_intro: ìš°ëŒ€ì¡°ê±´ ê°œìš”(introì •ë³´)
    chunk_type: (
        str  # basic_info, product_guide, rate_structure, preferential_conditions
    )
    chunk_index: int
    content_natural: str  # ë²¡í„° ê²€ìƒ‰ìš©(ë¬¸ì¥í˜•)
    content_structured: str  # llm ë¶„ì„ìš©(êµ¬ì¡°í™”)


class ProductChunksModel(BaseModel):
    """ìƒí’ˆë³„ ì²­í¬ ë°ì´í„° ëª¨ë¸"""

    product_code: str
    product_name: str
    chunks: list[ChunkModel]


class ParkingProductNLPConverter:
    def __init__(self, mongo_uri: str, db_name: str):
        """
        MongoDB ì—°ê²° ì´ˆê¸°í™”
        """
        self.client = pymongo.MongoClient(mongo_uri)
        self.db = self.client[db_name]
        self.products_details = self.db["product_details"]
        self.nlp_full = self.db["products_nlp_full"]
        self.nlp_chunks = self.db["products_nlp_chunks"]

    # START: ê³µí†µ í•¨ìˆ˜ë“¤
    # ì»¨ë²¤ì…˜ ê·œì¹™: private methodëŠ” ì•ì— '_'ì¶”ê°€
    @staticmethod
    def _extract_basic_info(product: dict[str, Any]) -> dict[str, Any]:
        """ìƒí’ˆ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        return {
            "product_name": product.get("product_name", "ì•Œ ìˆ˜ ì—†ëŠ” ìƒí’ˆ"),
            "company_name": product.get("company_name", "ì•Œ ìˆ˜ ì—†ëŠ” ì€í–‰"),
            "basic_rate": product.get("interest_rate", 0),
            "prime_rate": product.get("prime_interest_rate", 0),
            "rate_type": product.get("interest_guide", {}).get("rate_type", "ì¼ë°˜í˜•"),
            "categories": product.get("categories", []),
            "product_code": product.get("product_code", product.get("_id", "")),
        }

    @staticmethod
    def _extract_product_guide_info(product: dict[str, Any]) -> dict[str, str]:
        """ìƒí’ˆ ê°€ì´ë“œ ì •ë³´ ì¶”ì¶œ"""
        guide = product.get("product_guide", {})
        return {
            "target_customer": guide.get("target_customer", ""),
            "signup_method": guide.get("signup_method", ""),
            "amount_limit": guide.get("amount_limit", ""),
        }

    @staticmethod
    def _extract_basic_rate_info(product: dict[str, Any]) -> list[dict[str, str]]:
        """ê¸°ë³¸ ê¸ˆë¦¬ ì •ë³´ ì¶”ì¶œ"""
        interest_guide = product.get("interest_guide", {})
        return interest_guide.get("basic_rate_info", [])

    @staticmethod
    def _extract_preferential_details_info(product: dict[str, Any]) -> dict[str, Any]:
        """ìš°ëŒ€ì¡°ê±´ ì •ë³´ ì¶”ì¶œ"""
        interest_guide = product.get("interest_guide", {})
        preferential_details = interest_guide.get("preferential_details", {})
        return {
            "intro": preferential_details.get("intro", ""),
            "conditions": preferential_details.get("conditions", []),
        }

    # ì²­í¬ë³„ í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜ë“¤ (êµ¬ì¡°í™”, ìì—°ì–´ íŠœí”Œ ë°˜í™˜)
    @staticmethod
    def _generate_basic_info_content(basic_info: dict) -> tuple[str, str]:
        """ê¸°ë³¸ ì •ë³´ í…ìŠ¤íŠ¸ ìƒì„± (êµ¬ì¡°í™”, ìì—°ì–´)"""
        # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸
        structured = f"### ìƒí’ˆëª…: {basic_info['product_name']}\n"
        structured += f"### ì€í–‰: {basic_info['company_name']}\n"
        structured += f"### ê¸°ë³¸ ê¸ˆë¦¬: {basic_info['basic_rate']}%\n"
        structured += f"### ìµœê³  ìš°ëŒ€ê¸ˆë¦¬: {basic_info['prime_rate']}%\n"
        structured += f"### ê¸ˆë¦¬ ìœ í˜•: {basic_info['rate_type']}\n"
        structured += f"### ê°€ì… ë°©ì‹: {', '.join(basic_info['categories']) if basic_info['categories'] else 'ì—†ìŒ'}\n"

        # ìì—°ì–´ í…ìŠ¤íŠ¸
        natural = f"{basic_info['product_name']}ì€ {basic_info['company_name']}ì˜ {basic_info['rate_type']} íŒŒí‚¹í†µì¥ìœ¼ë¡œ "
        natural += f"ê¸°ë³¸ê¸ˆë¦¬ {basic_info['basic_rate']}%, ìµœê³ ìš°ëŒ€ê¸ˆë¦¬ {basic_info['prime_rate']}%ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
        if basic_info["categories"]:
            natural += f" ê°€ì…ë°©ì‹ì€ {', '.join(basic_info['categories'])}ì…ë‹ˆë‹¤."

        return structured, natural

    @staticmethod
    def _generate_product_guide_content(guide_info: dict) -> tuple[str, str]:
        """ìƒí’ˆ ê°€ì´ë“œ í…ìŠ¤íŠ¸ ìƒì„± (êµ¬ì¡°í™”, ìì—°ì–´)"""
        if not any(guide_info.values()):
            return "### ê°€ì…ì •ë³´: ì—†ìŒ\n", "ê°€ì…ì •ë³´ëŠ” ì—†ìŠµë‹ˆë‹¤."

        # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸
        structured = "### ê°€ì…ì •ë³´\n"
        if guide_info["target_customer"]:
            structured += f"- ê°€ì…ëŒ€ìƒ: {guide_info['target_customer']}\n"
        if guide_info["signup_method"]:
            structured += f"- ê°€ì…ë°©ë²•: {guide_info['signup_method']}\n"
        if guide_info["amount_limit"]:
            structured += f"- ê°€ì…í•œë„: {guide_info['amount_limit']}\n"

        # ìì—°ì–´ í…ìŠ¤íŠ¸
        guide_parts = []
        if guide_info["target_customer"]:
            guide_parts.append(f"ê°€ì…ëŒ€ìƒ: {guide_info['target_customer']}")
        if guide_info["signup_method"]:
            guide_parts.append(f"ê°€ì…ë°©ë²•: {guide_info['signup_method']}")
        if guide_info["amount_limit"]:
            guide_parts.append(f"ê°€ì…í•œë„: {guide_info['amount_limit']}")

        natural = "ê°€ì…ì •ë³´ - " + ", ".join(guide_parts)

        return structured, natural

    @staticmethod
    def _generate_basic_rate_content(rate_info: list) -> tuple[str, str]:
        """ê¸°ë³¸ ê¸ˆë¦¬ í…ìŠ¤íŠ¸ ìƒì„± (êµ¬ì¡°í™”, ìì—°ì–´)"""
        if not rate_info:
            return "### ê¸ˆë¦¬ êµ¬ê°„: ì—†ìŒ\n", "ê¸ˆë¦¬êµ¬ê°„ ì •ë³´ëŠ” ì—†ìŠµë‹ˆë‹¤."

        # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸
        structured = "### ê¸ˆë¦¬ êµ¬ê°„\n"
        for info in rate_info:
            if "condition" in info and "rate" in info:
                structured += f"- {info['condition']}: {info['rate']}\n"
            elif "text" in info:
                text_desc = info["text"].replace("\n", " ").strip()
                structured += f"- {text_desc}\n"

        # ìì—°ì–´ í…ìŠ¤íŠ¸
        rate_parts = []
        for info in rate_info:
            if "condition" in info and "rate" in info:
                rate_parts.append(f"{info['condition']}: {info['rate']}")
            elif "text" in info:
                rate_parts.append(info["text"].replace("\n", " ").strip())

        natural = "ê¸ˆë¦¬ì •ë³´ - " + ", ".join(rate_parts)

        return structured, natural

    @staticmethod
    def _generate_preferential_details_content(
        preferential_info: dict,
    ) -> tuple[str, str]:
        """ìš°ëŒ€ì¡°ê±´ í…ìŠ¤íŠ¸ ìƒì„± (êµ¬ì¡°í™”, ìì—°ì–´)"""
        intro = preferential_info["intro"]
        conditions = preferential_info["conditions"]

        # ìš°ëŒ€ì¡°ê±´ì´ ì—†ëŠ” ê²½ìš°
        if not intro and not conditions:
            return "### ìš°ëŒ€ì¡°ê±´: ì—†ìŒ\n", "ìš°ëŒ€ì¡°ê±´ì€ ì—†ìŠµë‹ˆë‹¤."

        # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸
        structured = "### ìš°ëŒ€ì¡°ê±´\n"
        if intro:
            structured += f"#### ê°œìš”\n- {intro}\n"

        if conditions:
            structured += "#### ì„¸ë¶€ì¡°ê±´\n"
            for condition in conditions:
                condition_idx = condition.get("index", "")
                desc = condition.get("description", "")
                if condition_idx and desc:
                    structured += f"{condition_idx}. {desc}\n"
                elif desc:
                    structured += f"- {desc}\n"

        # ìì—°ì–´ í…ìŠ¤íŠ¸
        natural = "ìš°ëŒ€ì¡°ê±´"
        if intro:
            natural += f" - ê°œìš”: {intro}"

        if conditions:
            natural += " - ì„¸ë¶€ì¡°ê±´: "
            condition_parts = []
            for condition in conditions:
                desc = condition.get("description", "").replace("\n", " ").strip()
                condition_idx = condition.get("index", "")
                if desc:
                    if condition_idx:
                        condition_parts.append(f"{condition_idx}.: {desc}")
                    else:
                        condition_parts.append(desc)
            natural += " / ".join(condition_parts)

        return structured, natural

    # END: ê³µí†µ í•¨ìˆ˜ë“¤

    def convert_to_full_document(self, product: dict[str, Any]) -> FullDocumentModel:
        """
        í†µì¥ ì •ë³´ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•íƒœì˜ ì „ì²´ ë¬¸ì„œë¡œ ë³€í™˜ (êµ¬ì¡°í™” + ìì—°ì–´ ëª¨ë‘ ìƒì„±)

        Args:
            product: products_details ì»¬ë ‰ì…˜ì˜ ë‹¨ì¼ ë¬¸ì„œ

        Returns:
            FullDocumentModel: ì „ì²´ ë¬¸ì„œ í˜•íƒœì˜ ìì—°ì–´ ë³€í™˜ ë°ì´í„°
        """
        # ê³µí†µ í•¨ìˆ˜ë¡œ ë°ì´í„° ì¶”ì¶œ
        basic_info = self._extract_basic_info(product)
        guide_info = self._extract_product_guide_info(product)
        rate_info = self._extract_basic_rate_info(product)
        preferential_info = self._extract_preferential_details_info(product)

        # ê° ì„¹ì…˜ë³„ í…ìŠ¤íŠ¸ ìƒì„±
        basic_structured, basic_natural = self._generate_basic_info_content(basic_info)
        guide_structured, guide_natural = self._generate_product_guide_content(
            guide_info
        )
        rate_structured, rate_natural = self._generate_basic_rate_content(rate_info)
        pref_structured, pref_natural = self._generate_preferential_details_content(
            preferential_info
        )

        # ì „ì²´ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì¡°í•©
        structured_content = f"## {basic_info['product_name']}\n"
        structured_content += basic_structured
        structured_content += guide_structured
        structured_content += rate_structured
        structured_content += pref_structured
        structured_content += "---\n"

        # ì „ì²´ ìì—°ì–´ í…ìŠ¤íŠ¸ ì¡°í•© (ìƒí’ˆëª… ì¤‘ë³µ ìµœì†Œí™”)
        natural_content = (
            f"{basic_natural} {guide_natural} {rate_natural} {pref_natural}"
        )

        return FullDocumentModel(
            _id=product.get("_id", ""),
            product_code=basic_info["product_code"],
            product_name=basic_info["product_name"],
            content_natural=natural_content.strip(),  # ë²¡í„° ê²€ìƒ‰ìš©
            content_structured=structured_content,  # LLM ë¶„ì„ìš©
        )

    def convert_to_chunks(self, product: dict[str, Any]) -> ProductChunksModel:
        """
        í†µì¥ ì •ë³´ë¥¼ ê¸°ëŠ¥ë³„ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ìƒí’ˆë³„ êµ¬ì¡°ë¡œ ë°˜í™˜

        Args:
            product: products_details ì»¬ë ‰ì…˜ì˜ ë‹¨ì¼ ë¬¸ì„œ

        Returns:
            ProductChunksModel: ìƒí’ˆ ì •ë³´ì™€ ì²­í¬ ë¦¬ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ë°ì´í„°
        """
        # ê³µí†µ í•¨ìˆ˜ë¡œ ë°ì´í„° ì¶”ì¶œ
        basic_info = self._extract_basic_info(product)
        guide_info = self._extract_product_guide_info(product)
        rate_info = self._extract_basic_rate_info(product)
        preferential_info = self._extract_preferential_details_info(product)

        chunks = []
        chunk_index = 1
        product_name = basic_info["product_name"]

        # ì²­í¬ 1: ê¸°ë³¸ ì •ë³´
        basic_structured, basic_natural = self._generate_basic_info_content(basic_info)
        chunks.append(
            ChunkModel(
                chunk_type="basic_info",
                chunk_index=chunk_index,
                content_structured=basic_structured,
                content_natural=basic_natural,  # ê¸°ë³¸ì •ë³´ëŠ” ìƒí’ˆëª…ì´ ì´ë¯¸ í¬í•¨ë¨
            )
        )
        chunk_index += 1

        # ì²­í¬ 2: ìƒí’ˆ ê°€ì´ë“œ ì •ë³´
        guide_structured, guide_natural = self._generate_product_guide_content(
            guide_info
        )
        # ìƒí’ˆëª…ì´ ì—†ëŠ” ê²½ìš° ì¶”ê°€
        if not guide_natural.startswith(product_name):
            guide_structured = f"##{product_name}\n {guide_structured}"
            guide_natural = f"{product_name} {guide_natural}"

        chunks.append(
            ChunkModel(
                chunk_type="product_guide",
                chunk_index=chunk_index,
                content_structured=guide_structured,
                content_natural=guide_natural,
            )
        )
        chunk_index += 1

        # ì²­í¬ 3: ê¸°ë³¸ ê¸ˆë¦¬ ì •ë³´
        rate_structured, rate_natural = self._generate_basic_rate_content(rate_info)
        # ìƒí’ˆëª…ì´ ì—†ëŠ” ê²½ìš° ì¶”ê°€
        if not rate_natural.startswith(product_name):
            rate_structured = f"##{product_name}\n {rate_structured}"
            rate_natural = f"{product_name}ì˜ {rate_natural}"

        chunks.append(
            ChunkModel(
                chunk_type="basic_rate_info",
                chunk_index=chunk_index,
                content_structured=rate_structured,
                content_natural=rate_natural,
            )
        )
        chunk_index += 1

        # ì²­í¬ 4: ìš°ëŒ€ì¡°ê±´ ì •ë³´
        pref_structured, pref_natural = self._generate_preferential_details_content(
            preferential_info
        )
        # ìƒí’ˆëª…ì´ ì—†ëŠ” ê²½ìš° ì¶”ê°€
        if not pref_natural.startswith(product_name):
            if pref_natural.startswith("ìš°ëŒ€ì¡°ê±´"):
                pref_structured = f"##{product_name}\n {pref_structured}"
                pref_natural = f"{product_name}ì˜ {pref_natural}"
            else:
                pref_structured = f"##{product_name}\n {pref_structured}"
                pref_natural = f"{product_name} {pref_natural}"

        chunks.append(
            ChunkModel(
                chunk_type="preferential_details",
                chunk_index=chunk_index,
                content_structured=pref_structured,
                content_natural=pref_natural,
            )
        )

        return ProductChunksModel(
            product_code=basic_info["product_code"],
            product_name=basic_info["product_name"],
            chunks=chunks,
        )

    def process_and_save(self):
        """
        products_detailsì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ ìì—°ì–´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        """
        # ê¸°ì¡´ NLP ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        self.nlp_full.drop()
        self.nlp_chunks.drop()

        print("ğŸ“Š products_detailsì—ì„œ ë°ì´í„° ì½ëŠ” ì¤‘...")
        products = list(self.products_details.find())
        print(f"ì´ {len(products)}ê°œ ìƒí’ˆ ë°œê²¬")

        full_documents = []
        chunk_documents = []  # ë³€ê²½: all_chunks -> chunk_documents

        for i, product in enumerate(products, 1):
            print(
                f"ì²˜ë¦¬ ì¤‘: {i}/{len(products)} - {product.get('product_name', 'Unknown')}"
            )

            # Full Document ë³€í™˜
            full_doc_model = self.convert_to_full_document(product)
            # Pydantic ëª¨ë¸ì„ dictë¡œ ë³€í™˜í•˜ê³  created_at ì¶”ê°€
            full_doc_dict = full_doc_model.model_dump(by_alias=True)
            full_doc_dict["created_at"] = datetime.now()
            full_documents.append(full_doc_dict)

            # Chunks ë³€í™˜ - ProductChunksModel ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ì €ì¥
            product_chunks_model = self.convert_to_chunks(product)
            # ProductChunksModelì„ dictë¡œ ë³€í™˜í•˜ê³  created_at ì¶”ê°€
            chunk_doc_dict = product_chunks_model.model_dump()
            chunk_doc_dict["created_at"] = datetime.now()
            chunk_documents.append(chunk_doc_dict)

        # MongoDBì— ì €ì¥
        print("\nğŸ’¾ Full Documents ì €ì¥ ì¤‘...")
        if full_documents:
            self.nlp_full.insert_many(full_documents)
            print(f"âœ… {len(full_documents)}ê°œ ì „ì²´ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")

        print("\nğŸ’¾ Chunk Documents ì €ì¥ ì¤‘...")
        if chunk_documents:
            self.nlp_chunks.insert_many(chunk_documents)
            print(f"âœ… {len(chunk_documents)}ê°œ ì²­í¬ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")

        # í†µê³„ ì¶œë ¥
        self.print_statistics()

    def print_statistics(self):
        """
        ë³€í™˜ ê²°ê³¼ í†µê³„ ì¶œë ¥
        """
        print("\nğŸ“ˆ ë³€í™˜ ê²°ê³¼ í†µê³„")
        print("=" * 50)

        # Full Documents í†µê³„
        full_count = self.nlp_full.count_documents({})
        print(f"ì „ì²´ ë¬¸ì„œ ìˆ˜: {full_count}")

        if full_count > 0:
            # í‰ê·  ë¬¸ì„œ ê¸¸ì´ (content_natural ê¸°ì¤€)
            pipeline = [
                {"$project": {"content_length": {"$strLenCP": "$content_natural"}}},
                {"$group": {"_id": None, "avg_length": {"$avg": "$content_length"}}},
            ]
            result = list(self.nlp_full.aggregate(pipeline))
            if result:
                print(f"í‰ê·  ë¬¸ì„œ ê¸¸ì´ (ìì—°ì–´): {result[0]['avg_length']:.0f} ê¸€ì")

        # Chunk Documents í†µê³„
        chunk_doc_count = self.nlp_chunks.count_documents({})
        print(f"ì²­í¬ ë¬¸ì„œ ìˆ˜: {chunk_doc_count}")

        if chunk_doc_count > 0:
            # ìƒí’ˆë‹¹ í‰ê·  ì²­í¬ ê°œìˆ˜
            pipeline = [
                {"$project": {"chunks_count": {"$size": "$chunks"}}},
                {"$group": {"_id": None, "avg_chunks": {"$avg": "$chunks_count"}}},
            ]
            result = list(self.nlp_chunks.aggregate(pipeline))
            if result:
                print(f"ìƒí’ˆë‹¹ í‰ê·  ì²­í¬ ê°œìˆ˜: {result[0]['avg_chunks']:.1f}ê°œ")

            # ì²­í¬ íƒ€ì…ë³„ ë¶„í¬
            pipeline = [
                {"$unwind": "$chunks"},
                {"$group": {"_id": "$chunks.chunk_type", "count": {"$sum": 1}}},
                {"$sort": {"count": -1}},
            ]
            chunk_stats = list(self.nlp_chunks.aggregate(pipeline))
            print("ì²­í¬ íƒ€ì…ë³„ ë¶„í¬:")
            for stat in chunk_stats:
                print(f"  - {stat['_id']}: {stat['count']}ê°œ")

        print("=" * 50)

    def sample_preview(self, limit: int = 3):
        """
        ë³€í™˜ ê²°ê³¼ ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°
        """
        print(f"\nğŸ” ë³€í™˜ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ {limit}ê°œ)")
        print("=" * 80)

        # Full Document ìƒ˜í”Œ
        print("ğŸ“„ Full Document ìƒ˜í”Œ:")
        for doc in self.nlp_full.find().limit(limit):
            print(f"\nìƒí’ˆëª…: {doc['product_name']}")
            print("ìì—°ì–´ ë‚´ìš©:")
            print(
                doc["content_natural"][:200] + "..."
                if len(doc["content_natural"]) > 200
                else doc["content_natural"]
            )
            print("-" * 40)

        # Chunk Documents ìƒ˜í”Œ
        print("\nğŸ§© Chunk Documents ìƒ˜í”Œ:")
        for chunk_doc in self.nlp_chunks.find().limit(limit):
            print(
                f"\nìƒí’ˆ: {chunk_doc['product_name']} | ìƒí’ˆì½”ë“œ: {chunk_doc['product_code']}"
            )
            print(f"ì²­í¬ ê°œìˆ˜: {len(chunk_doc['chunks'])}ê°œ")

            # ê° ì²­í¬ ë¯¸ë¦¬ë³´ê¸°
            for chunk in chunk_doc["chunks"]:
                print(
                    f"  - {chunk['chunk_type']} (#{chunk['chunk_index']}): {chunk['content_natural'][:100] + '...' if len(chunk['content_natural']) > 100 else chunk['content_natural']}"
                )
            print("-" * 40)


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    try:
        converter = ParkingProductNLPConverter(MONGO_URI, DB_NAME)
        print("ğŸš€ íŒŒí‚¹í†µì¥ ìì—°ì–´ ë³€í™˜ ì‹œì‘")
        converter.process_and_save()
        print("\nğŸš€ íŒŒí‚¹í†µì¥ ìì—°ì–´ ì €ì¥ ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
