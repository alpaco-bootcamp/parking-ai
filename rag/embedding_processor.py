"""
íŒŒí‚¹í†µì¥ ë°ì´í„° ë²¡í„°í™” ë° Pinecone ì €ì¥ ëª¨ë“ˆ

ë³¸ ëª¨ë“ˆì€ MongoDBì— ì €ì¥ëœ ìì—°ì–´ ë³€í™˜ ë°ì´í„°ë¥¼ OpenAI ì„ë² ë”©ìœ¼ë¡œ ë²¡í„°í™”í•˜ì—¬
ë³„ë„ì˜ Pinecone ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- content_natural ë²¡í„°í™”, content_structured ë©”íƒ€ë°ì´í„° ì €ì¥
"""

import os
import time
from enum import Enum

from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore

from common.enums import DocumentTypeEnum
from db.save_db import get_all_documents

load_dotenv()

# ì„¤ì • ìƒìˆ˜
EMBEDDING_MODEL = "text-embedding-3-small"
BATCH_SIZE = 50  # í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜ (4MB ì œí•œ ê³ ë ¤)
MAX_RETRIES = 3  # ì¬ì‹œë„ íšŸìˆ˜
RETRY_DELAY = 5  # ì¬ì‹œë„ ê°„ê²© (ì´ˆ)


class ProductsEmbeddingProcessor:
    """íŒŒí‚¹í†µì¥ ìƒí’ˆ ì„ë² ë”© ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        """
        ì„ë² ë”© í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”

        OpenAI ì„ë² ë”©ê³¼ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì¸ë±ìŠ¤ëª…ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        """

        # OpenAI ì„ë² ë”© ì´ˆê¸°í™”
        self.embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)

        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì¸ë±ìŠ¤ëª… ë¡œë“œ
        self.full_index_name = os.getenv("INDEX_NAME_FULL")
        self.chunks_index_name = os.getenv("INDEX_NAME_CHUNKS")

        print("âœ… ì…ë² ë”© í”„ë¡œì„¸ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    @staticmethod
    def _load_documents(collection_name: str) -> list[dict]:
        """
        ì»¬ë ‰ì…˜ì—ì„œ ë¬¸ì„œ ë°ì´í„° ë¡œë“œ

        Returns:
            list[dict]: ì „ì²´ ë¬¸ì„œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ“‚ {collection_name}ë°ì´í„° ë¡œë“œ ì¤‘..")
        documents = list(get_all_documents(collection_name))
        print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
        return documents

    @staticmethod
    def _convert_langchain_documents(
        documents: list[dict], doc_type: DocumentTypeEnum
    ) -> list[Document]:
        """
        MongoDBì—ì„œ ê°€ì ¸ì˜¨ íŒŒí‚¹í†µì¥ ë°ì´í„°ë¥¼ LangChain Document í˜•íƒœë¡œ ë³€í™˜

        ì´ í•¨ìˆ˜ëŠ” products_nlp_full ë˜ëŠ” products_nlp_chunks ì»¬ë ‰ì…˜ì˜ ë°ì´í„°ë¥¼
        LangChainì˜ Document ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ê° íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ë©”íƒ€ë°ì´í„° êµ¬ì¡°ì™€
        content ì²˜ë¦¬ ë°©ì‹ì„ ì ìš©í•©ë‹ˆë‹¤.

        Args:
            documents: MongoDBì—ì„œ ì¡°íšŒí•œ ë¬¸ì„œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            doc_type (DocumentTypeEnum): ë¬¸ì„œ íƒ€ì… Enum. 'full' ë˜ëŠ” 'chunks' ì¤‘ í•˜ë‚˜ë¥¼ ì§€ì •

        Returns:
            list[Document]: LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """

        langchain_documents = []
        total_chunks = 0

        for doc in documents:
            product_code = doc.get("product_code", "")
            product_name = doc.get("product_name", "")

            # ê³µí†µ ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                "product_code": product_code,
                "product_name": product_name,
                "doc_type": doc_type,
                "text": ",",  # ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©
                "content_structured": "",  # LLMìš”ì²­ìš© ë¬¸ì„œ ë‚´ìš©
            }

            # full Typeì¸ ê²½ìš°
            if doc_type == DocumentTypeEnum.FULL:
                content_natural = doc.get("content_natural", "")  # ë²¡í„° ê²€ìƒ‰ìš©
                content_structured = doc.get("content_structured", "")  # llm ìš”ì²­ìš©
                if not content_natural:
                    print(
                        f"âš ï¸ {doc.get('product_name', 'Unknown')} - content_naturalì´ ë¹„ì–´ìˆìŒ"
                    )
                    continue

                # Full ë©”íƒ€ë°ì´í„° êµ¬ì„±
                metadata.update(
                    {"text": content_natural, "content_structured": content_structured}
                )

                # LangChain Document ìƒì„±
                document = Document(page_content=content_natural, metadata=metadata)

                langchain_documents.append(document)

            # chunksì¸ ê²½ìš°
            if doc_type == DocumentTypeEnum.CHUNKS:
                chunks = doc.get("chunks", [])

                if not chunks:
                    print(f"âš ï¸ {product_name} - ì²­í¬ê°€ ë¹„ì–´ìˆìŒ")
                    continue

                # ì²­í¬ ë°ì´í„° ìˆœíšŒ
                for chunk in chunks:
                    total_chunks += 1

                    content_natural = chunk.get("content_natural", "")  # ë²¡í„° ê²€ìƒ‰ìš©
                    content_structured = chunk.get(
                        "content_structured", ""
                    )  # llm ìš”ì²­ìš©

                    if not content_natural:
                        print(
                            f"âš ï¸ {doc.get('product_name', 'Unknown')} - content_naturalì´ ë¹„ì–´ìˆìŒ"
                        )
                        continue

                    # ğŸ“Œ ìì—°ì–´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                    # Chunks ë©”íƒ€ë°ì´í„° êµ¬ì„±
                    metadata.update(
                        {
                            "text": content_natural,
                            "content_structured": content_structured,
                            "chunk_type": chunk.get("chunk_type", ""),
                            "chunk_index": chunk.get("chunk_index", 0),
                        }
                    )

                    # LangChain Document ìƒì„±
                    document = Document(page_content=content_natural, metadata=metadata)
                    langchain_documents.append(document)

            print(f"  âœ“ {product_name} - Document ìƒì„± ì™„ë£Œ")

        print(f"ğŸ“Š ì´ {total_chunks}ê°œ ì²­í¬ Document ìƒì„± ì™„ë£Œ")
        return langchain_documents

    # ProductsEmbeddingProcessor í´ë˜ìŠ¤ì— ì¶”ê°€í•  ë©”ì„œë“œ
    def clear_pinecone_index(self, index_name: str) -> None:
        """
        Pinecone ì¸ë±ìŠ¤ì˜ ëª¨ë“  ë²¡í„°ë¥¼ ì‚­ì œ (LangChain PineconeVectorStore ì‚¬ìš©)

        Args:
            index_name: ì´ˆê¸°í™”í•  Pinecone ì¸ë±ìŠ¤ëª…
        """
        try:
            print(f"ğŸ—‘ï¸ {index_name} ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì¤‘...")

            # LangChain PineconeVectorStoreë¡œ ì¸ë±ìŠ¤ ì—°ê²°
            vector_store = PineconeVectorStore(
                embedding=self.embeddings, index_name=index_name
            )

            # ì¸ë±ìŠ¤ì˜ ëª¨ë“  ë²¡í„° ì‚­ì œ
            vector_store.delete(delete_all=True)

            print(f"âœ… {index_name} ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

            # ì‚­ì œ ì™„ë£Œë¥¼ ìœ„í•œ ì ì‹œ ëŒ€ê¸°
            import time

            time.sleep(2)

        except Exception as e:
            print(f"âš ï¸ {index_name} ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            print("â„¹ï¸ ê¸°ì¡´ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    def batch_upload_to_pinecone(
        self, documents: list[Document], index_name: str
    ) -> None:
        """
        ë¬¸ì„œë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ Pineconeì— ì—…ë¡œë“œ

        Args:
            documents: ì—…ë¡œë“œí•  Document ë¦¬ìŠ¤íŠ¸
            index_name: Pinecone ì¸ë±ìŠ¤ëª…
        """
        total_docs = len(documents)
        total_batches = (total_docs + BATCH_SIZE - 1) // BATCH_SIZE

        print(f"ğŸ“Š ì´ {total_docs}ê°œ ë¬¸ì„œë¥¼ {total_batches}ê°œ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì—…ë¡œë“œ")

        successful_uploads = 0
        failed_uploads = 0

        for batch_idx in range(total_batches):
            start_idx = batch_idx * BATCH_SIZE
            end_idx = min(start_idx + BATCH_SIZE, total_docs)
            batch_documents = documents[start_idx:end_idx]

            batch_size = len(batch_documents)
            print(
                f"\nğŸ”„ ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì²˜ë¦¬ ì¤‘... ({batch_size}ê°œ ë¬¸ì„œ)"
            )

            # ì¬ì‹œë„ ë¡œì§
            for attempt in range(MAX_RETRIES):
                try:
                    # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ë¬¸ì„œ ì¶”ê°€
                    if batch_idx == 0:
                        # ì²« ë²ˆì§¸ ë°°ì¹˜ëŠ” ìƒˆë¡œìš´ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
                        vector_store = PineconeVectorStore.from_documents(
                            documents=batch_documents,
                            embedding=self.embeddings,
                            index_name=index_name,
                        )
                    else:
                        # ì´í›„ ë°°ì¹˜ëŠ” ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€
                        vector_store = PineconeVectorStore(
                            embedding=self.embeddings, index_name=index_name
                        )
                        vector_store.add_documents(batch_documents)

                    successful_uploads += batch_size
                    print(f"âœ… ë°°ì¹˜ {batch_idx + 1} ì—…ë¡œë“œ ì„±ê³µ ({batch_size}ê°œ)")
                    break

                except Exception as e:
                    attempt_msg = f"ì‹œë„ {attempt + 1}/{MAX_RETRIES}"
                    print(
                        f"âŒ ë°°ì¹˜ {batch_idx + 1} ì—…ë¡œë“œ ì‹¤íŒ¨ ({attempt_msg}): {str(e)}"
                    )

                    if attempt < MAX_RETRIES - 1:
                        print(f"â³ {RETRY_DELAY}ì´ˆ í›„ ì¬ì‹œë„...")
                        time.sleep(RETRY_DELAY)
                    else:
                        print(f"ğŸ’¥ ë°°ì¹˜ {batch_idx + 1} ìµœì¢… ì‹¤íŒ¨")
                        failed_uploads += batch_size

            # ì§„í–‰ë¥  í‘œì‹œ
            progress = ((batch_idx + 1) / total_batches) * 100
            print(
                f"ğŸ“ˆ ì „ì²´ ì§„í–‰ë¥ : {progress:.1f}% ({successful_uploads}/{total_docs})"
            )

        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š ì—…ë¡œë“œ ì™„ë£Œ!")
        print(f"  âœ… ì„±ê³µ: {successful_uploads}ê°œ")
        print(f"  âŒ ì‹¤íŒ¨: {failed_uploads}ê°œ")
        print(f"  ğŸ“ˆ ì„±ê³µë¥ : {(successful_uploads / total_docs) * 100:.1f}%")

    def process_vector_store(
        self, documents: list[dict], doc_type: DocumentTypeEnum
    ) -> None:
        """
        ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ì—¬ Pineconeì— ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            documents (list[dict]): ì»¬ë ‰ì…˜ì—ì„œ ë¡œë“œí•œ ë°ì´í„°
            doc_type (DocumentTypeEnum): ë¬¸ì„œ íƒ€ì… Enum. 'full' ë˜ëŠ” 'chunks' ì¤‘ í•˜ë‚˜ë¥¼ ì§€ì •
        """

        print(f"\nğŸ”„ ì „ì²´ ë¬¸ì„œ ë²¡í„°í™” ì‹œì‘ ({len(documents)}ê°œ)")

        # LangChain Document í˜•íƒœë¡œ ë³€í™˜
        langchain_documents = self._convert_langchain_documents(documents, doc_type)
        index_name = (
            self.full_index_name
            if doc_type == DocumentTypeEnum.FULL
            else self.chunks_index_name
        )

        if not langchain_documents:
            print(f"âŒ {doc_type}: ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
            return

        # PineconeVectorStoreë¡œ ë²¡í„°í™” ë° ì €ì¥
        print(
            f"\nğŸ’¾ {len(langchain_documents)}ê°œ ë¬¸ì„œë¥¼ {index_name}ì— ë°°ì¹˜ ì—…ë¡œë“œ ì¤‘..."
        )

        # ğŸ”„ ìƒˆë¡œ ì¶”ê°€: ê¸°ì¡´ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.clear_pinecone_index(index_name)

        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì—…ë¡œë“œ
        self.batch_upload_to_pinecone(langchain_documents, index_name)

    def load_vector_store(self, doc_type: DocumentTypeEnum) -> PineconeVectorStore:
        """
        doc_typeì— ë§ëŠ” PineconeVectorStore ê°ì²´ë¥¼ ë°˜í™˜

        Args:
            doc_type (DocumentTypeEnum): ë¬¸ì„œ íƒ€ì… Enum. 'full' ë˜ëŠ” 'chunks' ì¤‘ í•˜ë‚˜ë¥¼ ì§€ì •

        Returns:
            PineconeVectorStore: ì§€ì •ëœ ì¸ë±ìŠ¤ì˜ ë²¡í„°ìŠ¤í† ì–´ ê°ì²´
        """
        print(f"ğŸ”Œ {doc_type} ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì¤‘...")

        index_name = (
            self.full_index_name
            if doc_type == DocumentTypeEnum.FULL
            else self.chunks_index_name
        )
        vector_store = PineconeVectorStore(
            embedding=self.embeddings, index_name=index_name
        )

        print(f"âœ… {doc_type} ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì™„ë£Œ")
        print("index_name: ", index_name)
        print("doc_type: ", doc_type)
        return vector_store

    def process_all_data(self) -> None:
        """
        ì „ì²´ ë°ì´í„° ë¡œë“œ ë° ë²¡í„°í™” ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜

        MongoDBì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë‘ ê°œì˜ Pinecone ì¸ë±ìŠ¤ì— ê°ê° ì €ì¥í•©ë‹ˆë‹¤.
        """

        try:
            print("ğŸš€ íŒŒí‚¹í†µì¥ ë°ì´í„° ë²¡í„°í™” ì‹œì‘")
            print("=" * 60)

            # 1. ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬
            full_documents = self._load_documents("products_nlp_full")
            self.process_vector_store(
                documents=full_documents, doc_type=DocumentTypeEnum.FULL
            )

            # 2. ì²­í¬ ë¬¸ì„œ ì²˜ë¦¬
            chunks_documents = self._load_documents("products_nlp_chunks")
            self.process_vector_store(
                documents=chunks_documents, doc_type=DocumentTypeEnum.CHUNKS
            )

        except Exception as e:
            print(f"âš ï¸ ë²¡í„°ìŠ¤í† ì–´ ì²˜ë¦¬ì¤‘ ì˜¤ë¥˜ {e}")


if __name__ == "__main__":
    embedding_processor = ProductsEmbeddingProcessor()
    # ë²¡í„°í™” ë° ì €ì¥
    # embedding_processor.process_all_data()

    # ë²¡í„°ìŠ¤í† ì–´ ë¶ˆëŸ¬ì˜¤ê¸°
    full_vector_store = embedding_processor.load_vector_store(DocumentTypeEnum.FULL)
    chunk_vector_store = embedding_processor.load_vector_store(DocumentTypeEnum.CHUNKS)
    print(f"full_vector_store: {full_vector_store}")
    print(f"chunk_vector_store: {chunk_vector_store}")
