"""
íŒŒí‚¹í†µì¥ ë°ì´í„° ë²¡í„°í™” ë° Pinecone ì €ì¥ ëª¨ë“ˆ

ë³¸ ëª¨ë“ˆì€ MongoDBì— ì €ì¥ëœ ìì—°ì–´ ë³€í™˜ ë°ì´í„°ë¥¼ OpenAI ì„ë² ë”©ìœ¼ë¡œ ë²¡í„°í™”í•˜ì—¬
ë³„ë„ì˜ Pinecone ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- content_natural ë²¡í„°í™”, content_structured ë©”íƒ€ë°ì´í„° ì €ì¥
"""

import os
from enum import Enum

from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore

from common.enums import DocumentTypeEnum
from db.save_db import get_all_documents

load_dotenv()

# ì„¤ì • ìƒìˆ˜
EMBEDDING_MODEL = "text-embedding-3-small"

class ProductsEmbeddingProcessor:
    """íŒŒí‚¹í†µì¥ ìƒí’ˆ ì„ë² ë”© ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        """
        ì„ë² ë”© í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”

        OpenAI ì„ë² ë”©ê³¼ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì¸ë±ìŠ¤ëª…ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        """

        # OpenAI ì„ë² ë”© ì´ˆê¸°í™”
        self.embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)

        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì¸ë±ìŠ¤ëª… ë¡œë“œ
        self.full_index_name = os.getenv("INDEX_NAME_FULL")
        self.chunks_index_name = os.getenv("INDEX_NAME_CHUNKS")

        print("âœ… ì…ë² ë”© í”„ë¡œì„¸ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    @staticmethod
    def _load_documents(collection_name: str) -> list[dict]:
        """
        ì»¬ë ‰ì…˜ì—ì„œ ë¬¸ì„œ ë°ì´í„° ë¡œë“œ

        Returns:
            list[dict]: ì „ì²´ ë¬¸ì„œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ“‚ {collection_name}ë°ì´í„° ë¡œë“œ ì¤‘..")
        documents = list(get_all_documents(collection_name))
        print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
        return documents

    @staticmethod
    def _convert_langchain_documents(
        documents: list[dict], doc_type: DocumentTypeEnum
    ) -> list[Document]:
        """
        MongoDBì—ì„œ ê°€ì ¸ì˜¨ íŒŒí‚¹í†µì¥ ë°ì´í„°ë¥¼ LangChain Document í˜•íƒœë¡œ ë³€í™˜

        ì´ í•¨ìˆ˜ëŠ” products_nlp_full ë˜ëŠ” products_nlp_chunks ì»¬ë ‰ì…˜ì˜ ë°ì´í„°ë¥¼
        LangChainì˜ Document ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ê° íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ë©”íƒ€ë°ì´í„° êµ¬ì¡°ì™€
        content ì²˜ë¦¬ ë°©ì‹ì„ ì ìš©í•©ë‹ˆë‹¤.

        Args:
            documents: MongoDBì—ì„œ ì¡°íšŒí•œ ë¬¸ì„œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            doc_type (DocumentTypeEnum): ë¬¸ì„œ íƒ€ì… Enum. 'full' ë˜ëŠ” 'chunks' ì¤‘ í•˜ë‚˜ë¥¼ ì§€ì •

        Returns:
            list[Document]: LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """

        langchain_documents = []
        total_chunks = 0

        for doc in documents:
            product_code = doc.get("product_code", "")
            product_name = doc.get("product_name", "")

            # ê³µí†µ ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                "product_code": product_code,
                "product_name": product_name,
                "doc_type": doc_type,
                "text": ",",  # ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©
                "content_structured": "",  # LLMìš”ì²­ìš© ë¬¸ì„œ ë‚´ìš©
            }

            # full Typeì¸ ê²½ìš°
            if doc_type == DocumentTypeEnum.FULL:
                content_natural = doc.get("content_natural", "")  # ë²¡í„° ê²€ìƒ‰ìš©
                content_structured = doc.get("content_structured", "")  # llm ìš”ì²­ìš©
                if not content_natural:
                    print(
                        f"âš ï¸ {doc.get('product_name', 'Unknown')} - content_naturalì´ ë¹„ì–´ìˆìŒ"
                    )
                    continue

                # Full ë©”íƒ€ë°ì´í„° êµ¬ì„±
                metadata.update(
                    {"text": content_natural, "content_structured": content_structured}
                )

                # LangChain Document ìƒì„±
                document = Document(page_content=content_natural, metadata=metadata)

                langchain_documents.append(document)

            # chunksì¸ ê²½ìš°
            if doc_type == DocumentTypeEnum.CHUNKS:
                chunks = doc.get("chunks", [])

                if not chunks:
                    print(f"âš ï¸ {product_name} - ì²­í¬ê°€ ë¹„ì–´ìˆìŒ")
                    continue

                # ì²­í¬ ë°ì´í„° ìˆœíšŒ
                for chunk in chunks:
                    total_chunks += 1

                    content_natural = chunk.get("content_natural", "")  # ë²¡í„° ê²€ìƒ‰ìš©
                    content_structured = chunk.get(
                        "content_structured", ""
                    )  # llm ìš”ì²­ìš©

                    if not content_natural:
                        print(
                            f"âš ï¸ {doc.get('product_name', 'Unknown')} - content_naturalì´ ë¹„ì–´ìˆìŒ"
                        )
                        continue

                    # ğŸ“Œ ìì—°ì–´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                    # Chunks ë©”íƒ€ë°ì´í„° êµ¬ì„±
                    metadata.update(
                        {
                            "text": content_natural,
                            "content_structured": content_structured,
                            "chunk_type": chunk.get("chunk_type", ""),
                            "chunk_index": chunk.get("chunk_index", 0),
                        }
                    )

                    # LangChain Document ìƒì„±
                    document = Document(page_content=content_natural, metadata=metadata)
                    langchain_documents.append(document)


            print(f'ğŸ“€ {doc_type} metatdata: {metadata}')
            print(f"  âœ“ {product_name} - Document ìƒì„± ì™„ë£Œ")

        print(f"ğŸ“Š ì´ {total_chunks}ê°œ ì²­í¬ Document ìƒì„± ì™„ë£Œ")
        return langchain_documents

    def process_vector_store(self, documents: list[dict], doc_type: DocumentTypeEnum) -> None:
        """
    ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ì—¬ Pineconeì— ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        documents (list[dict]): ì»¬ë ‰ì…˜ì—ì„œ ë¡œë“œí•œ ë°ì´í„°
        doc_type (DocumentTypeEnum): ë¬¸ì„œ íƒ€ì… Enum. 'full' ë˜ëŠ” 'chunks' ì¤‘ í•˜ë‚˜ë¥¼ ì§€ì •
    """

        print(f"\nğŸ”„ ì „ì²´ ë¬¸ì„œ ë²¡í„°í™” ì‹œì‘ ({len(documents)}ê°œ)")

        # LangChain Document í˜•íƒœë¡œ ë³€í™˜
        documents = self._convert_langchain_documents(documents, doc_type)
        index_name = self.full_index_name if doc_type == DocumentTypeEnum.FULL else self.chunks_index_name

        if not documents:
            print(f"âŒ {doc_type}: ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
            return

        # PineconeVectorStoreë¡œ ë²¡í„°í™” ë° ì €ì¥
        print(f"\nğŸ’¾ {len(documents)}ê°œ ë¬¸ì„œë¥¼ {index_name}ì— ë²¡í„°í™” ë° ì €ì¥ ì¤‘...")

        vector_store = PineconeVectorStore.from_documents(
            documents=documents,
            embedding=self.embeddings,
            index_name=index_name
        )

        print(f"âœ… ì „ì²´ ë¬¸ì„œ ë²¡í„° ì €ì¥ ì™„ë£Œ ({len(documents)}ê°œ)")

    def load_vector_store(self, doc_type: DocumentTypeEnum) -> PineconeVectorStore:
        """
                doc_typeì— ë§ëŠ” PineconeVectorStore ê°ì²´ë¥¼ ë°˜í™˜

                Args:
                    doc_type (DocumentTypeEnum): ë¬¸ì„œ íƒ€ì… Enum. 'full' ë˜ëŠ” 'chunks' ì¤‘ í•˜ë‚˜ë¥¼ ì§€ì •

                Returns:
                    PineconeVectorStore: ì§€ì •ëœ ì¸ë±ìŠ¤ì˜ ë²¡í„°ìŠ¤í† ì–´ ê°ì²´
                """
        print(f"ğŸ”Œ {doc_type} ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì¤‘...")

        index_name = self.full_index_name if doc_type == DocumentTypeEnum.FULL else self.chunks_index_name
        vector_store = PineconeVectorStore(embedding=self.embeddings, index_name=index_name)

        print(f"âœ… {doc_type} ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì™„ë£Œ")
        print('index_name: ', index_name)
        print('doc_type: ', doc_type)
        return vector_store



    def process_all_data(self) -> None:
        """
        ì „ì²´ ë°ì´í„° ë¡œë“œ ë° ë²¡í„°í™” ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜

        MongoDBì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë‘ ê°œì˜ Pinecone ì¸ë±ìŠ¤ì— ê°ê° ì €ì¥í•©ë‹ˆë‹¤.
        """

        try:
            print("ğŸš€ íŒŒí‚¹í†µì¥ ë°ì´í„° ë²¡í„°í™” ì‹œì‘")
            print("=" * 60)

            # 1. ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬
            # full_documents = self._load_documents('products_nlp_full')
            # self.process_vector_store(documents=full_documents, doc_type=DocumentTypeEnum.FULL)

            # 2. ì²­í¬ ë¬¸ì„œ ì²˜ë¦¬
            chunks_documents = self._load_documents('products_nlp_chunks')
            self.process_vector_store(documents=chunks_documents, doc_type=DocumentTypeEnum.CHUNKS)

        except Exception as e:
            print(f'âš ï¸ ë²¡í„°ìŠ¤í† ì–´ ì²˜ë¦¬ì¤‘ ì˜¤ë¥˜ {e}')


if __name__ == "__main__":
    embedding_processor = ProductsEmbeddingProcessor()
    # ë²¡í„°í™” ë° ì €ì¥
    # embedding_processor.process_all_data()

    # ë²¡í„°ìŠ¤í† ì–´ ë¶ˆëŸ¬ì˜¤ê¸°
    # full_vector_store = embedding_processor.load_vector_store(DocumentTypeEnum.FULL)
    chunk_vector_store = embedding_processor.load_vector_store(DocumentTypeEnum.CHUNKS)
    # print(f"chunk_vector_store: {full_vector_store}")
