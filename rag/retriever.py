"""
RAG ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬í˜„

ì´ ëª¨ë“ˆì€ Pinecone ë²¡í„°ìŠ¤í† ì–´ë¥¼ í™œìš©í•˜ì—¬ íŒŒí‚¹í†µì¥ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ê³ ,
full ë²¡í„°ìŠ¤í† ì–´ì™€ chunks ë²¡í„°ìŠ¤í† ì–´ì˜ ê²€ìƒ‰ í’ˆì§ˆì„ ë¹„êµí•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- Full ë¬¸ì„œ ê²€ìƒ‰ ë° LLM ì‘ë‹µ
- Chunks ë¬¸ì„œ ê²€ìƒ‰ ë° LLM ì‘ë‹µ
- page_content vs content_structured ë¹„êµ
"""

from dataclasses import dataclass
from enum import Enum

from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document

from common.enums import DocumentTypeEnum
from rag.embedding_processor import ProductsEmbeddingProcessor
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()

@dataclass
class ContentTypeEnum(str, Enum):
    """LLMì— ì „ë‹¬í•  ì»¨í…ì¸  íƒ€ì…"""

    PAGE_CONTENT = "page_content"  # ë²¡í„°í™”ëœ ìì—°ì–´ í…ìŠ¤íŠ¸ (ê¸°ë³¸ LangChain ë°©ì‹)
    CONTENT_STRUCTURED = "content_structured"  # êµ¬ì¡°í™”ëœ ë©”íƒ€ë°ì´í„° í…ìŠ¤íŠ¸

class ParkingRetriever:
    """íŒŒí‚¹í†µì¥ RAG ê²€ìƒ‰ ì‹œìŠ¤í…œ"""

    def __init__(self):
        """ê²€ìƒ‰ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.embedding_processor = ProductsEmbeddingProcessor()
        self.embeddings = self.embedding_processor.embeddings

        # LLM ì´ˆê¸°í™”
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)

        # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
        self.full_vector_store = None
        self.chunks_vector_store = None

        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì •ì˜
        self.test_queries = [
            {
                "query": "7% ê³ ê¸ˆë¦¬ íŒŒí‚¹í†µì¥ ì°¾ê³  ìˆì–´ìš”. ìš°ëŒ€ê¸ˆë¦¬ë„ ê°€ëŠ¥í•´ìš”",
                "expected_chunks": ["basic_rate_info", "preferential_details"],
                "description": "ê³ ê¸ˆë¦¬ ì¡°ê±´ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸",
            },
            {
                "query": "ë¹„ëŒ€ë©´ìœ¼ë¡œ ê°€ì…í•  ìˆ˜ ìˆëŠ” í†µì¥ì´ ìˆë‚˜ìš”?",
                "expected_chunks": ["product_guide", "preferential_details"],
                "description": "ê°€ì… ë°©ë²• ê²€ìƒ‰ í…ŒìŠ¤íŠ¸",
            },
            {
                "query": "ë§ˆì¼€íŒ… ìˆ˜ì‹  ë™ì˜í•˜ë©´ ìš°ëŒ€ê¸ˆë¦¬ ë°›ì„ ìˆ˜ ìˆëŠ” ê³³ì€?",
                "expected_chunks": ["preferential_details"],
                "description": "ìš°ëŒ€ì¡°ê±´ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸",
            },
            # {
            #     "query": "OKì €ì¶•ì€í–‰ íŒŒí‚¹í†µì¥ ê¸ˆë¦¬ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            #     "expected_chunks": ["basic_info", "basic_rate_info"],
            #     "description": "íŠ¹ì • ì€í–‰ ìƒí’ˆ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸",
            # },
            # {
            #     "query": "1000ë§Œì› ë„£ì„ ìˆ˜ ìˆëŠ” íŒŒí‚¹í†µì¥ ì¶”ì²œí•´ì£¼ì„¸ìš”",
            #     "expected_chunks": ["product_guide", "basic_rate_info"],
            #     "description": "ì˜ˆì¹˜ í•œë„ ê¸°ë°˜ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸",
            # },
        ]

        print("âœ… ParkingRetriever ì´ˆê¸°í™” ì™„ë£Œ")

    def load_vector_stores(self):
        """ë²¡í„°ìŠ¤í† ì–´ ì§€ì—° ë¡œë”©"""
        if self.full_vector_store is None:
            self.full_vector_store = self.embedding_processor.load_vector_store(
                DocumentTypeEnum.FULL
            )
        if self.chunks_vector_store is None:
            self.chunks_vector_store = self.embedding_processor.load_vector_store(
                DocumentTypeEnum.CHUNKS
            )

    def llm_with_full(
        self, query: str, k: int = 5, use_structured: bool = False
    ) -> str:
        """
        Full ë²¡í„°ìŠ¤í† ì–´ë¡œ ê²€ìƒ‰í•˜ì—¬ LLM ì‘ë‹µ ìƒì„±

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            use_structured: Trueë©´ content_structured, Falseë©´ page_content ì‚¬ìš©

        Returns:
            str: LLM ì‘ë‹µ
        """
        self.load_vector_stores()

        print(f"ğŸ“– Full ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ ì¤‘... (k={k})")

        # 1. ë²¡í„° ê²€ìƒ‰
        docs_with_scores = self.full_vector_store.similarity_search_with_score(
            query, k=k
        )

        # 2. content_structured ì‚¬ìš©í•˜ëŠ” ê²½ìš° page_content êµì²´
        if use_structured:
            docs = []
            for doc, score in docs_with_scores:
                structured_content = doc.metadata.get(
                    "content_structured", doc.page_content
                )
                new_doc = Document(
                    page_content=structured_content, metadata=doc.metadata
                )
                docs.append(new_doc)
            content_type = ContentTypeEnum.CONTENT_STRUCTURED
        else:
            docs = [doc for doc, score in docs_with_scores]
            content_type = ContentTypeEnum.PAGE_CONTENT

        # 3. LLM ì‘ë‹µ ìƒì„±
        return self.generate_llm_response(query, docs, content_type, "Full")

    def llm_with_chunks(
        self, query: str, k: int = 10, use_structured: bool = False
    ) -> str:
        """
        Chunks ë²¡í„°ìŠ¤í† ì–´ë¡œ ê²€ìƒ‰í•˜ì—¬ LLM ì‘ë‹µ ìƒì„±

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            use_structured: Trueë©´ content_structured, Falseë©´ page_content ì‚¬ìš©

        Returns:
            str: LLM ì‘ë‹µ
        """
        self.load_vector_stores()

        print(f"ğŸ“ Chunks ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ ì¤‘... (k={k})")

        # 1. ë²¡í„° ê²€ìƒ‰
        docs_with_scores = self.chunks_vector_store.similarity_search_with_score(
            query, k=k
        )

        # 2. content_structured ì‚¬ìš©í•˜ëŠ” ê²½ìš° page_content êµì²´
        if use_structured:
            docs = []
            for doc, score in docs_with_scores:
                structured_content = doc.metadata.get(
                    "content_structured", doc.page_content
                )
                new_doc = Document(
                    page_content=structured_content, metadata=doc.metadata
                )
                docs.append(new_doc)
            content_type = ContentTypeEnum.CONTENT_STRUCTURED
        else:
            docs = [doc for doc, score in docs_with_scores]
            content_type = ContentTypeEnum.PAGE_CONTENT

        # 3. LLM ì‘ë‹µ ìƒì„±
        return self.generate_llm_response(query, docs, content_type, "Chunks")

    @staticmethod
    def _format_docs(documents: list[Document]) -> str:
        """
        Document ë¦¬ìŠ¤íŠ¸ë¥¼ LLM ì…ë ¥ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…

        Args:
            documents: LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸

        Returns:
            str: í¬ë§·íŒ…ëœ ë¬¸ì„œ í…ìŠ¤íŠ¸
        """
        return "\n\n".join([doc.page_content for doc in documents])

    def generate_llm_response(
        self,
        query: str,
        documents: list[Document],
        content_type: ContentTypeEnum,
        doc_source: str,
    ) -> str:
        """
        LCEL ë°©ì‹ìœ¼ë¡œ LLM ì‘ë‹µ ìƒì„±

        Args:
            query: ì‚¬ìš©ì ì§ˆì˜
            documents: LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
            content_type: ì»¨í…ì¸  íƒ€ì…
            doc_source: ë¬¸ì„œ ì†ŒìŠ¤ (Full ë˜ëŠ” Chunks)

        Returns:
            str: LLMì´ ìƒì„±í•œ íŒŒí‚¹í†µì¥ ì¶”ì²œ ì „ëµ
        """


        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        prompt_template = ChatPromptTemplate.from_template(
            """ë‹¹ì‹ ì€ íŒŒí‚¹í†µì¥ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆì˜ì— ëŒ€í•´ ì ì ˆí•œ íŒŒí‚¹í†µì¥ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

    **ê²€ìƒ‰ ë°©ì‹**: {doc_source} ë²¡í„°ìŠ¤í† ì–´
    **ì»¨í…ì¸  íƒ€ì…**: {content_type}
    **ì‚¬ìš©ì ì§ˆì˜**: {query}

    **ê²€ìƒ‰ëœ íŒŒí‚¹í†µì¥ ì •ë³´**:
    {context}

    **ì‘ë‹µ ìš”êµ¬ì‚¬í•­**:
    1. ì§ˆì˜ì— ê°€ì¥ ì í•©í•œ íŒŒí‚¹í†µì¥ 2-3ê°œë¥¼ ì¶”ì²œí•˜ì„¸ìš”
    2. ê° ìƒí’ˆì˜ ì£¼ìš” íŠ¹ì§•ê³¼ ì¥ì ì„ ì„¤ëª…í•˜ì„¸ìš”  
    3. ê¸ˆë¦¬ ì •ë³´ì™€ ìš°ëŒ€ì¡°ê±´ì„ í¬í•¨í•˜ì„¸ìš”
    4. ê°„ê²°í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”

    **ì¶”ì²œ íŒŒí‚¹í†µì¥**:"""
        )

        # Context ë°ì´í„° ìƒì„±í•˜ì—¬ ì¶œë ¥
        context_data = self._format_docs(documents)
        print(f"\nğŸ“„ ê²€ìƒ‰ëœ Context ë°ì´í„°:")
        print("-" * 60)
        print(context_data)
        print("-" * 60)

        # LCEL ì²´ì¸ êµ¬ì„±
        chain = (
            {
                "context": RunnableLambda(lambda x: x["documents"]) | RunnableLambda(self._format_docs),  # documents â†’ format_docs
                "query": lambda x: x["query"],
                "doc_source": lambda x: x["doc_source"],
                "content_type": lambda x: x["content_type"],
            }
            | prompt_template
            | self.llm
            | StrOutputParser()  # ë¬¸ìì—´ ì¶œë ¥ìœ¼ë¡œ íŒŒì‹±
        )

        try:
            # ì²´ì¸ ì‹¤í–‰
            response = chain.invoke(
                {
                    "documents": documents,
                    "query": query,
                    "doc_source": doc_source,
                    "content_type": content_type.value,
                }
            )
            return response # ë¬¸ìì—´
        except Exception as e:
            return f"[{doc_source} - {content_type.value} ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}]"

    def run_comparison_test(self, query: str, k_full: int = 5, k_chunks: int = 10):
        """
        ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ 4ê°€ì§€ ë°©ì‹ ë¹„êµ í…ŒìŠ¤íŠ¸
        â€¢ Full ë²¡í„°ìŠ¤í† ì–´ + page_content (ìì—°ì–´ ê²€ìƒ‰ ê²°ê³¼)
        â€¢ Full ë²¡í„°ìŠ¤í† ì–´ + content_structured (êµ¬ì¡°í™”ëœ ê²€ìƒ‰ ê²°ê³¼)
        â€¢ Chunks ë²¡í„°ìŠ¤í† ì–´ + page_content (ì²­í¬ë³„ ìì—°ì–´ ê²€ìƒ‰ ê²°ê³¼)
        â€¢ Chunks ë²¡í„°ìŠ¤í† ì–´ + content_structured (ì²­í¬ë³„ êµ¬ì¡°í™”ëœ ê²€ìƒ‰ ê²°ê³¼)

        Args:
            query: í…ŒìŠ¤íŠ¸í•  ì¿¼ë¦¬
            k_full: Full ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜
            k_chunks: Chunks ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜
        """
        print(f"\nğŸ” ë¹„êµ í…ŒìŠ¤íŠ¸: '{query}'")
        print("=" * 80)

        # 1. Full + page_content
        print("\n1ï¸âƒ£ Full ë²¡í„°ìŠ¤í† ì–´ + page_content")
        print("-" * 40)
        full_page_answer = self.llm_with_full(query, k=k_full, use_structured=False)
        print(f'ğŸ”¥Full + page_contentë‹µë³€: \n {full_page_answer}')

        # 2. Full + content_structured
        print("\n2ï¸âƒ£ Full ë²¡í„°ìŠ¤í† ì–´ + content_structured")
        print("-" * 40)
        full_structured_answer = self.llm_with_full(
            query, k=k_full, use_structured=True
        )
        print(f'ğŸ”¥Full + content_structuredë‹µë³€: \n {full_structured_answer}')

        # 3. Chunks + page_content
        print("\n3ï¸âƒ£ Chunks ë²¡í„°ìŠ¤í† ì–´ + page_content")
        print("-" * 40)
        chunks_page_answer = self.llm_with_chunks(
            query, k=k_chunks, use_structured=False
        )
        print(f'ğŸ”¥Chunks + page_contentë‹µë³€: \n {chunks_page_answer}')

        # 4. Chunks + content_structured
        print("\n4ï¸âƒ£ Chunks ë²¡í„°ìŠ¤í† ì–´ + content_structured")
        print("-" * 40)
        chunks_structured_answer = self.llm_with_chunks(
            query, k=k_chunks, use_structured=True
        )
        print(f'ğŸ”¥Chunks + content_structuredë‹µë³€: \n {chunks_structured_answer}')

    def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ì— ëŒ€í•´ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ íŒŒí‚¹í†µì¥ ê²€ìƒ‰ í’ˆì§ˆ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 80)

        for i, test_case in enumerate(self.test_queries, 1):
            print(f"\n[í…ŒìŠ¤íŠ¸ {i}/{len(self.test_queries)}] {test_case['description']}")
            self.run_comparison_test(test_case["query"])

            if i < len(self.test_queries):
                print("\n" + "=" * 80)


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    retriever = ParkingRetriever()

    # ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    retriever.run_all_tests()

    # íŠ¹ì • ì¿¼ë¦¬ë§Œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´:
    # retriever.run_comparison_test("7% ê³ ê¸ˆë¦¬ íŒŒí‚¹í†µì¥ ì°¾ê³  ìˆì–´ìš”")

    # ê°œë³„ í•¨ìˆ˜ ì‚¬ìš©í•˜ë ¤ë©´:
    # full_answer = retriever.llm_with_full("ê³ ê¸ˆë¦¬ íŒŒí‚¹í†µì¥", k=3, use_structured=True)
    # chunks_answer = retriever.llm_with_chunks("ê³ ê¸ˆë¦¬ íŒŒí‚¹í†µì¥", k=8, use_structured=False)
